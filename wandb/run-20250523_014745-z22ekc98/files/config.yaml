_current_progress_remaining:
    value: 1
_custom_logger:
    value: "False"
_episode_num:
    value: 0
_last_episode_starts:
    value: |-
        [ True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True]
_last_obs:
    value: |-
        [[ 0.97901887 -0.5524642   0.11767878 ...  0.          0.
           0.        ]
         [ 0.12466492  1.1332223  -1.2380902  ...  0.          0.
           0.        ]
         [-0.74248207  1.1524365   1.1148103  ...  0.          0.
           0.        ]
         ...
         [ 1.3866404  -0.50660557 -0.37603387 ...  0.          0.
           0.        ]
         [ 0.01297359 -1.5682851  -1.2074374  ...  0.          0.
           0.        ]
         [-0.7989266   1.2692648   0.45954177 ...  0.          0.
           0.        ]]
_last_original_obs:
    value: |-
        [[ 1.40516025e+00  9.96872985e-01  3.38094988e-04 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39972790e+00  1.00794453e+00 -7.62101702e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39421422e+00  1.00807073e+00  6.19180792e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         ...
         [ 1.40775208e+00  9.97174183e-01 -2.56027078e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39901772e+00  9.90201098e-01 -7.44106855e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39385532e+00  1.00883806e+00  2.34501947e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]]
_logger:
    value: <stable_baselines3.common.logger.Logger object at 0x347c5efd0>
_n_updates:
    value: 0
_num_timesteps_at_start:
    value: 0
_stats_window_size:
    value: 100
_total_timesteps:
    value: 6553600
_vec_normalize_env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x15b63c710>
_wandb:
    value:
        cli_version: 0.19.11
        code_path: code/src/train_humanoid_ppo.py
        m: []
        python_version: 3.11.11
        t:
            "1":
                - 1
                - 55
            "2":
                - 1
                - 55
            "3":
                - 1
                - 2
                - 3
                - 13
                - 22
                - 23
                - 35
                - 55
            "4": 3.11.11
            "5": 0.19.11
            "8":
                - 5
            "12": 0.19.11
            "13": darwin-arm64
action_noise:
    value: None
action_space:
    value: Box(-0.4, 0.4, (17,), float32)
algo:
    value: PPO
batch_size:
    value: 256
clip_range:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x3470f25c0>
clip_range_vf:
    value: None
device:
    value: cpu
ent_coef:
    value: 0.01
env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x15b63c710>
ep_info_buffer:
    value: deque([], maxlen=100)
ep_success_buffer:
    value: deque([], maxlen=100)
gae_lambda:
    value: 0.95
gamma:
    value: 0.995
learning_rate:
    value: 0.0001
lr_schedule:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x154f09940>
max_grad_norm:
    value: 0.5
n_envs:
    value: 32
n_epochs:
    value: 100
n_steps:
    value: 2048
normalize_advantage:
    value: "True"
num_timesteps:
    value: 0
observation_space:
    value: Box(-inf, inf, (348,), float64)
policy:
    value: |-
        ActorCriticPolicy(
          (features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (pi_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (vf_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (mlp_extractor): MlpExtractor(
            (policy_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
            )
            (value_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
            )
          )
          (action_net): Linear(in_features=512, out_features=17, bias=True)
          (value_net): Linear(in_features=512, out_features=1, bias=True)
        )
policy_class:
    value: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>
policy_kwargs:
    value: '{''net_arch'': [{''pi'': [512, 512], ''vf'': [512, 512]}]}'
rollout_buffer:
    value: <stable_baselines3.common.buffers.RolloutBuffer object at 0x15a54b390>
rollout_buffer_class:
    value: <class 'stable_baselines3.common.buffers.RolloutBuffer'>
rollout_buffer_kwargs:
    value: '{}'
sde_sample_freq:
    value: -1
seed:
    value: None
start_time:
    value: 1747990067327411000
target_kl:
    value: None
tensorboard_log:
    value: ./runs/ppo_humanoid/20250523-014744
use_sde:
    value: "False"
verbose:
    value: 1
vf_coef:
    value: 0.5
