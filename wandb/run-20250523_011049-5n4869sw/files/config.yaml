_current_progress_remaining:
    value: 1
_custom_logger:
    value: "False"
_episode_num:
    value: 0
_last_episode_starts:
    value: |-
        [ True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True]
_last_obs:
    value: |-
        [[-0.21997403  0.11970995  0.1174597  ...  0.          0.
           0.        ]
         [-0.742937    1.0675693  -0.755254   ...  0.          0.
           0.        ]
         [-0.16203335  1.2389859   1.1648624  ...  0.          0.
           0.        ]
         ...
         [ 0.85205334  0.13908376 -1.078393   ...  0.          0.
           0.        ]
         [ 0.45613712  1.1690218   1.5239896  ...  0.          0.
           0.        ]
         [-0.6461397   0.20834254 -0.8937352  ...  0.          0.
           0.        ]]
_last_original_obs:
    value: |-
        [[ 1.39992984e+00  1.00197573e+00  1.53649516e-04 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39668537e+00  1.00788967e+00 -3.92871238e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.40028931e+00  1.00895918e+00  5.05316825e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         ...
         [ 1.40658071e+00  1.00209661e+00 -5.44028549e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.40412444e+00  1.00852266e+00  6.73308547e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39728591e+00  1.00252873e+00 -4.57649681e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]]
_logger:
    value: <stable_baselines3.common.logger.Logger object at 0x326a45f50>
_n_updates:
    value: 0
_num_timesteps_at_start:
    value: 0
_stats_window_size:
    value: 100
_total_timesteps:
    value: 655360
_vec_normalize_env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x17784ca10>
_wandb:
    value:
        cli_version: 0.19.11
        code_path: code/src/train_humanoid_ppo.py
        m: []
        python_version: 3.11.11
        t:
            "1":
                - 1
                - 55
            "2":
                - 1
                - 55
            "3":
                - 1
                - 13
                - 22
                - 23
                - 35
                - 55
            "4": 3.11.11
            "5": 0.19.11
            "8":
                - 5
            "12": 0.19.11
            "13": darwin-arm64
action_noise:
    value: None
action_space:
    value: Box(-0.4, 0.4, (17,), float32)
algo:
    value: PPO
batch_size:
    value: 256
clip_range:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x3365f6340>
clip_range_vf:
    value: None
device:
    value: cpu
ent_coef:
    value: 0.01
env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x17784ca10>
ep_info_buffer:
    value: deque([], maxlen=100)
ep_success_buffer:
    value: deque([], maxlen=100)
gae_lambda:
    value: 0.95
gamma:
    value: 0.995
learning_rate:
    value: 0.0001
lr_schedule:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x1659f1940>
max_grad_norm:
    value: 0.5
n_envs:
    value: 32
n_epochs:
    value: 1000
n_steps:
    value: 2048
normalize_advantage:
    value: "True"
num_timesteps:
    value: 0
observation_space:
    value: Box(-inf, inf, (348,), float64)
policy:
    value: |-
        ActorCriticPolicy(
          (features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (pi_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (vf_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (mlp_extractor): MlpExtractor(
            (policy_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
              (4): Linear(in_features=512, out_features=512, bias=True)
              (5): Tanh()
            )
            (value_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
              (4): Linear(in_features=512, out_features=512, bias=True)
              (5): Tanh()
            )
          )
          (action_net): Linear(in_features=512, out_features=17, bias=True)
          (value_net): Linear(in_features=512, out_features=1, bias=True)
        )
policy_class:
    value: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>
policy_kwargs:
    value: '{''net_arch'': [{''pi'': [512, 512, 512], ''vf'': [512, 512, 512]}]}'
rollout_buffer:
    value: <stable_baselines3.common.buffers.RolloutBuffer object at 0x17fae8150>
rollout_buffer_class:
    value: <class 'stable_baselines3.common.buffers.RolloutBuffer'>
rollout_buffer_kwargs:
    value: '{}'
sde_sample_freq:
    value: -1
seed:
    value: None
start_time:
    value: 1747987850782820000
target_kl:
    value: None
tensorboard_log:
    value: ./runs/ppo_humanoid/20250523-011049
use_sde:
    value: "False"
verbose:
    value: 1
vf_coef:
    value: 0.5
