diff --git a/src/train_humanoid_ppo.py b/src/train_humanoid_ppo.py
index 09b7b67..981cbf5 100644
--- a/src/train_humanoid_ppo.py
+++ b/src/train_humanoid_ppo.py
@@ -1,5 +1,6 @@
 import gym
 import time
+import torch
 from stable_baselines3 import PPO
 from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
 from stable_baselines3.common.env_util import make_vec_env
@@ -9,31 +10,40 @@ import wandb
 from wandb.integration.sb3 import WandbCallback
 
 env_id = "Humanoid-v5"  
-num_envs = 4  
+num_envs = 32
 
 env = make_vec_env(env_id, n_envs=num_envs)
 env = VecNormalize(env, norm_obs=True, norm_reward=True)
 timestamp = time.strftime("%Y%m%d-%H%M%S")
-wandb.init(project="ppo_humanoid", name=timestamp)
+wandb.init(
+    name=f"PPO_{timestamp}", 
+    sync_tensorboard=True,  
+    monitor_gym=True,       
+    save_code=True,
+)
+
+device = "cuda" if torch.cuda.is_available() else "cpu"
 
 model = PPO(
     "MlpPolicy",        
     env,
+    device=device,
     verbose=1,
-    learning_rate=3e-4,
+    learning_rate=1e-4,
     n_steps=2048,        
-    batch_size=64,
-    n_epochs=10,
-    gamma=0.99,
+    batch_size=256,
+    n_epochs=1000,
+    gamma=0.995,
     gae_lambda=0.95,
     clip_range=0.2,
     ent_coef=0.01,
     vf_coef=0.5,
     max_grad_norm=0.5,
+    policy_kwargs=dict(net_arch=[dict(pi=[512, 512, 512], vf=[512, 512, 512])]),
     tensorboard_log=f"./runs/ppo_humanoid/{timestamp}"
 )
 
-total_timesteps = 5_000_000
+total_timesteps = 655360
 model.learn(
     total_timesteps=total_timesteps,
     callback=WandbCallback(
