_current_progress_remaining:
    value: 1
_custom_logger:
    value: "False"
_episode_num:
    value: 0
_last_episode_starts:
    value: |-
        [ True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True  True  True  True  True
          True  True  True  True  True  True  True  True]
_last_obs:
    value: |-
        [[ 0.06170591  0.7114388  -0.02382003 ...  0.          0.
           0.        ]
         [-0.09092187 -1.5117475   0.7098305  ...  0.          0.
           0.        ]
         [-0.68274736  1.6236452  -0.3536127  ...  0.          0.
           0.        ]
         ...
         [-1.0468732  -0.5350759  -1.0303321  ...  0.          0.
           0.        ]
         [-0.13125713 -1.0974777  -1.1676953  ...  0.          0.
           0.        ]
         [-0.45691243 -1.3547553   0.12595561 ...  0.          0.
           0.        ]]
_last_original_obs:
    value: |-
        [[ 1.40118068e+00  1.00414358e+00 -1.34833723e-04 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.40027532e+00  9.90403564e-01  4.08545369e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39676469e+00  1.00978131e+00 -2.03194919e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         ...
         [ 1.39460475e+00  9.96439712e-01 -5.92474312e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.40003605e+00  9.92963886e-01 -6.71491811e-03 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]
         [ 1.39810431e+00  9.91373827e-01  7.26743083e-04 ...  0.00000000e+00
           0.00000000e+00  0.00000000e+00]]
_logger:
    value: <stable_baselines3.common.logger.Logger object at 0x335352f90>
_n_updates:
    value: 0
_num_timesteps_at_start:
    value: 0
_stats_window_size:
    value: 100
_total_timesteps:
    value: 655360
_vec_normalize_env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x16a2a8690>
_wandb:
    value:
        cli_version: 0.19.11
        code_path: code/src/train_humanoid_ppo.py
        m: []
        python_version: 3.11.11
        t:
            "1":
                - 1
                - 55
            "2":
                - 1
                - 55
            "3":
                - 1
                - 13
                - 22
                - 23
                - 35
                - 55
            "4": 3.11.11
            "5": 0.19.11
            "8":
                - 5
            "12": 0.19.11
            "13": darwin-arm64
action_noise:
    value: None
action_space:
    value: Box(-0.4, 0.4, (17,), float32)
algo:
    value: PPO
batch_size:
    value: 256
clip_range:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x3347eed40>
clip_range_vf:
    value: None
device:
    value: cpu
ent_coef:
    value: 0.01
env:
    value: <stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x16a2a8690>
ep_info_buffer:
    value: deque([], maxlen=100)
ep_success_buffer:
    value: deque([], maxlen=100)
gae_lambda:
    value: 0.95
gamma:
    value: 0.995
learning_rate:
    value: 0.0001
lr_schedule:
    value: <function get_schedule_fn.<locals>.<lambda> at 0x16c3a93a0>
max_grad_norm:
    value: 0.5
n_envs:
    value: 32
n_epochs:
    value: 1000
n_steps:
    value: 2048
normalize_advantage:
    value: "True"
num_timesteps:
    value: 0
observation_space:
    value: Box(-inf, inf, (348,), float64)
policy:
    value: |-
        ActorCriticPolicy(
          (features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (pi_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (vf_features_extractor): FlattenExtractor(
            (flatten): Flatten(start_dim=1, end_dim=-1)
          )
          (mlp_extractor): MlpExtractor(
            (policy_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
              (4): Linear(in_features=512, out_features=512, bias=True)
              (5): Tanh()
            )
            (value_net): Sequential(
              (0): Linear(in_features=348, out_features=512, bias=True)
              (1): Tanh()
              (2): Linear(in_features=512, out_features=512, bias=True)
              (3): Tanh()
              (4): Linear(in_features=512, out_features=512, bias=True)
              (5): Tanh()
            )
          )
          (action_net): Linear(in_features=512, out_features=17, bias=True)
          (value_net): Linear(in_features=512, out_features=1, bias=True)
        )
policy_class:
    value: <class 'stable_baselines3.common.policies.ActorCriticPolicy'>
policy_kwargs:
    value: '{''net_arch'': [{''pi'': [512, 512, 512], ''vf'': [512, 512, 512]}]}'
rollout_buffer:
    value: <stable_baselines3.common.buffers.RolloutBuffer object at 0x315dda5d0>
rollout_buffer_class:
    value: <class 'stable_baselines3.common.buffers.RolloutBuffer'>
rollout_buffer_kwargs:
    value: '{}'
sde_sample_freq:
    value: -1
seed:
    value: None
start_time:
    value: 1747987716214832000
target_kl:
    value: None
tensorboard_log:
    value: ./runs/ppo_humanoid/20250523-010819
use_sde:
    value: "False"
verbose:
    value: 1
vf_coef:
    value: 0.5
