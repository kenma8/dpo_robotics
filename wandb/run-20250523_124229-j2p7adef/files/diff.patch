diff --git a/ppo_humanoid.zip b/ppo_humanoid.zip
index 40dc611..e9e72db 100644
Binary files a/ppo_humanoid.zip and b/ppo_humanoid.zip differ
diff --git a/requirements.txt b/requirements.txt
index cc4f90e..c7317fd 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -6,7 +6,7 @@ matplotlib==3.5.3
 ipython==6.4.0
 moviepy==1.0.0
 pyvirtualdisplay==1.3.2
-torch==1.13.1
+torch
 opencv-python
 networkx==2.5
 ipdb==0.13.3
@@ -15,4 +15,6 @@ scipy
 numpy==1.26
 minari
 stable-baselines3
-wandb
\ No newline at end of file
+wandb
+mujoco
+tensorboard
\ No newline at end of file
diff --git a/src/train_humanoid_ppo.py b/src/train_humanoid_ppo.py
index 981cbf5..b140d1c 100644
--- a/src/train_humanoid_ppo.py
+++ b/src/train_humanoid_ppo.py
@@ -10,7 +10,7 @@ import wandb
 from wandb.integration.sb3 import WandbCallback
 
 env_id = "Humanoid-v5"  
-num_envs = 32
+num_envs = 4
 
 env = make_vec_env(env_id, n_envs=num_envs)
 env = VecNormalize(env, norm_obs=True, norm_reward=True)
@@ -29,21 +29,21 @@ model = PPO(
     env,
     device=device,
     verbose=1,
-    learning_rate=1e-4,
+    learning_rate=3e-4,
     n_steps=2048,        
-    batch_size=256,
-    n_epochs=1000,
+    batch_size=64,
+    n_epochs=10,
     gamma=0.995,
     gae_lambda=0.95,
     clip_range=0.2,
     ent_coef=0.01,
     vf_coef=0.5,
     max_grad_norm=0.5,
-    policy_kwargs=dict(net_arch=[dict(pi=[512, 512, 512], vf=[512, 512, 512])]),
+    policy_kwargs=dict(net_arch=[dict(pi=[512, 512], vf=[512, 512])]),
     tensorboard_log=f"./runs/ppo_humanoid/{timestamp}"
 )
 
-total_timesteps = 655360
+total_timesteps = 10000000
 model.learn(
     total_timesteps=total_timesteps,
     callback=WandbCallback(
diff --git a/vec_normalize.pkl b/vec_normalize.pkl
index 0d24b7b..562a03f 100644
Binary files a/vec_normalize.pkl and b/vec_normalize.pkl differ
diff --git a/wandb/debug-internal.log b/wandb/debug-internal.log
index 3d9c54c..a052d2f 120000
--- a/wandb/debug-internal.log
+++ b/wandb/debug-internal.log
@@ -1 +1 @@
-run-20250523_011049-5n4869sw/logs/debug-internal.log
\ No newline at end of file
+run-20250523_124229-j2p7adef/logs/debug-internal.log
\ No newline at end of file
diff --git a/wandb/debug.log b/wandb/debug.log
index f3ae627..709d698 120000
--- a/wandb/debug.log
+++ b/wandb/debug.log
@@ -1 +1 @@
-run-20250523_011049-5n4869sw/logs/debug.log
\ No newline at end of file
+run-20250523_124229-j2p7adef/logs/debug.log
\ No newline at end of file
diff --git a/wandb/latest-run b/wandb/latest-run
index 9ef0d73..0d28a5a 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250523_011049-5n4869sw
\ No newline at end of file
+run-20250523_124229-j2p7adef
\ No newline at end of file
diff --git a/wandb/run-20250523_011049-5n4869sw/files/output.log b/wandb/run-20250523_011049-5n4869sw/files/output.log
index 72266b3..5efa454 100644
--- a/wandb/run-20250523_011049-5n4869sw/files/output.log
+++ b/wandb/run-20250523_011049-5n4869sw/files/output.log
@@ -12,3 +12,48 @@ Logging to ./runs/ppo_humanoid/20250523-011049/PPO_1
 |    time_elapsed    | 12       |
 |    total_timesteps | 65536    |
 ---------------------------------
+Traceback (most recent call last):
+  File "/Users/kenma/Documents/dpo_robotics/src/train_humanoid_ppo.py", line 47, in <module>
+    model.learn(
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
+    return super().learn(
+           ^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 337, in learn
+    self.train()
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py", line 213, in train
+    values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)
+                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/common/policies.py", line 732, in evaluate_actions
+    latent_pi, latent_vf = self.mlp_extractor(features)
+                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
+    return self._call_impl(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
+    return forward_call(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/common/torch_layers.py", line 257, in forward
+    return self.forward_actor(features), self.forward_critic(features)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/stable_baselines3/common/torch_layers.py", line 260, in forward_actor
+    return self.policy_net(features)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
+    return self._call_impl(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
+    return forward_call(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/container.py", line 240, in forward
+    input = module(input)
+            ^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
+    return self._call_impl(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
+    return forward_call(*args, **kwargs)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+  File "/opt/miniconda3/envs/cs224r/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 125, in forward
+    return F.linear(input, self.weight, self.bias)
+           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
+KeyboardInterrupt
diff --git a/wandb/run-20250523_011049-5n4869sw/logs/debug-internal.log b/wandb/run-20250523_011049-5n4869sw/logs/debug-internal.log
index 2fa34c6..279de2b 100644
--- a/wandb/run-20250523_011049-5n4869sw/logs/debug-internal.log
+++ b/wandb/run-20250523_011049-5n4869sw/logs/debug-internal.log
@@ -7,3 +7,12 @@
 {"time":"2025-05-23T01:10:49.913957-07:00","level":"INFO","msg":"Starting system monitor"}
 {"time":"2025-05-23T01:10:49.933387-07:00","level":"ERROR","msg":"error getting latest commit","error":"exit status 128"}
 {"time":"2025-05-23T01:11:00.788172-07:00","level":"INFO","msg":"tensorboard: no root directory after 10 seconds, using working directory"}
+{"time":"2025-05-23T01:19:08.228286-07:00","level":"INFO","msg":"api: retrying error","error":"Post \"https://api.wandb.ai/graphql\": read tcp 10.27.182.197:50200->35.186.228.49:443: read: operation timed out"}
+{"time":"2025-05-23T01:21:55.416309-07:00","level":"INFO","msg":"stream: closing","id":"5n4869sw"}
+{"time":"2025-05-23T01:21:55.416793-07:00","level":"INFO","msg":"Stopping system monitor"}
+{"time":"2025-05-23T01:21:55.417093-07:00","level":"INFO","msg":"Stopped system monitor"}
+{"time":"2025-05-23T01:21:55.887255-07:00","level":"INFO","msg":"fileTransfer: Close: file transfer manager closed"}
+{"time":"2025-05-23T01:21:56.044101-07:00","level":"INFO","msg":"handler: closed","stream_id":"5n4869sw"}
+{"time":"2025-05-23T01:21:56.04509-07:00","level":"INFO","msg":"sender: closed","stream_id":"5n4869sw"}
+{"time":"2025-05-23T01:21:56.045134-07:00","level":"INFO","msg":"writer: Close: closed","stream_id":"5n4869sw"}
+{"time":"2025-05-23T01:21:56.045691-07:00","level":"INFO","msg":"stream: closed","id":"5n4869sw"}
diff --git a/wandb/run-20250523_011049-5n4869sw/logs/debug.log b/wandb/run-20250523_011049-5n4869sw/logs/debug.log
index f66b4d7..9054009 100644
--- a/wandb/run-20250523_011049-5n4869sw/logs/debug.log
+++ b/wandb/run-20250523_011049-5n4869sw/logs/debug.log
@@ -23,3 +23,4 @@ config: {'_wandb': {'code_path': 'code/src/train_humanoid_ppo.py'}}
 2025-05-23 01:10:50,786 INFO    MainThread:81097 [wandb_run.py:_tensorboard_callback():1645] tensorboard callback: ./runs/ppo_humanoid/20250523-011049/PPO_1, True
 2025-05-23 01:10:50,787 INFO    MainThread:81097 [wandb_watch.py:_watch():71] Watching
 2025-05-23 01:10:50,788 INFO    MainThread:81097 [wandb_run.py:_config_callback():1436] config_cb None None {'algo': 'PPO', 'policy_class': "<class 'stable_baselines3.common.policies.ActorCriticPolicy'>", 'device': 'cpu', 'verbose': 1, 'policy_kwargs': "{'net_arch': [{'pi': [512, 512, 512], 'vf': [512, 512, 512]}]}", 'num_timesteps': 0, '_total_timesteps': 655360, '_num_timesteps_at_start': 0, 'seed': 'None', 'action_noise': 'None', 'start_time': 1747987850782820000, 'learning_rate': 0.0001, 'tensorboard_log': './runs/ppo_humanoid/20250523-011049', '_last_obs': '[[-0.21997403  0.11970995  0.1174597  ...  0.          0.\n   0.        ]\n [-0.742937    1.0675693  -0.755254   ...  0.          0.\n   0.        ]\n [-0.16203335  1.2389859   1.1648624  ...  0.          0.\n   0.        ]\n ...\n [ 0.85205334  0.13908376 -1.078393   ...  0.          0.\n   0.        ]\n [ 0.45613712  1.1690218   1.5239896  ...  0.          0.\n   0.        ]\n [-0.6461397   0.20834254 -0.8937352  ...  0.          0.\n   0.        ]]', '_last_episode_starts': '[ True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True  True  True  True  True\n  True  True  True  True  True  True  True  True]', '_last_original_obs': '[[ 1.39992984e+00  1.00197573e+00  1.53649516e-04 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [ 1.39668537e+00  1.00788967e+00 -3.92871238e-03 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [ 1.40028931e+00  1.00895918e+00  5.05316825e-03 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n ...\n [ 1.40658071e+00  1.00209661e+00 -5.44028549e-03 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [ 1.40412444e+00  1.00852266e+00  6.73308547e-03 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]\n [ 1.39728591e+00  1.00252873e+00 -4.57649681e-03 ...  0.00000000e+00\n   0.00000000e+00  0.00000000e+00]]', '_episode_num': 0, 'use_sde': 'False', 'sde_sample_freq': -1, '_current_progress_remaining': 1.0, '_stats_window_size': 100, 'ep_info_buffer': 'deque([], maxlen=100)', 'ep_success_buffer': 'deque([], maxlen=100)', '_n_updates': 0, '_custom_logger': 'False', 'env': '<stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x17784ca10>', '_vec_normalize_env': '<stable_baselines3.common.vec_env.vec_normalize.VecNormalize object at 0x17784ca10>', 'observation_space': 'Box(-inf, inf, (348,), float64)', 'action_space': 'Box(-0.4, 0.4, (17,), float32)', 'n_envs': 32, 'n_steps': 2048, 'gamma': 0.995, 'gae_lambda': 0.95, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'rollout_buffer_class': "<class 'stable_baselines3.common.buffers.RolloutBuffer'>", 'rollout_buffer_kwargs': '{}', 'batch_size': 256, 'n_epochs': 1000, 'clip_range': '<function get_schedule_fn.<locals>.<lambda> at 0x3365f6340>', 'clip_range_vf': 'None', 'normalize_advantage': 'True', 'target_kl': 'None', 'lr_schedule': '<function get_schedule_fn.<locals>.<lambda> at 0x1659f1940>', 'rollout_buffer': '<stable_baselines3.common.buffers.RolloutBuffer object at 0x17fae8150>', 'policy': 'ActorCriticPolicy(\n  (features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (pi_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (vf_features_extractor): FlattenExtractor(\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n  )\n  (mlp_extractor): MlpExtractor(\n    (policy_net): Sequential(\n      (0): Linear(in_features=348, out_features=512, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=512, out_features=512, bias=True)\n      (3): Tanh()\n      (4): Linear(in_features=512, out_features=512, bias=True)\n      (5): Tanh()\n    )\n    (value_net): Sequential(\n      (0): Linear(in_features=348, out_features=512, bias=True)\n      (1): Tanh()\n      (2): Linear(in_features=512, out_features=512, bias=True)\n      (3): Tanh()\n      (4): Linear(in_features=512, out_features=512, bias=True)\n      (5): Tanh()\n    )\n  )\n  (action_net): Linear(in_features=512, out_features=17, bias=True)\n  (value_net): Linear(in_features=512, out_features=1, bias=True)\n)', '_logger': '<stable_baselines3.common.logger.Logger object at 0x326a45f50>'}
+2025-05-23 01:21:55,412 INFO    MsgRouterThr:81097 [mailbox.py:close():129] [no run ID] Closing mailbox, abandoning 1 handles.
diff --git a/wandb/run-20250523_011049-5n4869sw/run-5n4869sw.wandb b/wandb/run-20250523_011049-5n4869sw/run-5n4869sw.wandb
index b50c1e5..33db342 100644
Binary files a/wandb/run-20250523_011049-5n4869sw/run-5n4869sw.wandb and b/wandb/run-20250523_011049-5n4869sw/run-5n4869sw.wandb differ
